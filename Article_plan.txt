1. Введение (аналогично OneMax+ZeroMax)
	а) Про многокритериалку
	б) EA+RL и про прошлые работы по нему (все были по нестрого монотонным функциям)
	в) интерес к jump-функции с теоретической точки зрения (зачем считаем время работы), обзор литературы
2. Постановка задачи
	а) Фитнес-функция и вспомогательные критерии
	б) описание алгоритма (в тч Q-learning и перезапуски)
3. Время работы алгоритма без перезапусков
	а) Разбиение хода алгоритма на 3 этапа
	б) Оценка времени работы первого этапа системой или дрифтом (лучше дрифтом)
	в) Рассуждение про окончание первого этапа: 2 варианта выбора оптимизируемого критерия (jump или right_bridge)
		- Хороший вариант
			- Рассмотрение 2ого этапа при выборе right_bridge. Аналог Learning lemma.
			- Рассмотрение 3его этапа при выборе right_bridge (должно занять пару строчек)
		- Плохой вариант 
			- Обоснование того, что при выборе jump алгоритм зациклится. 
4. Время работы с перезапусками		
		- Напоминание про важность выбора функции при переходе на второй этап
	а) При переходе на второй этап по целевой функции
		- Матожидание времени до перезапуска: второй этап проходится так же быстро, но надо показать, сколько итераций займет двойное падение и подъем назад
	б) При переходе на второй этап по right_bridge
		-Вероятность того, что при выборе right_bridge произойдет перезапуск.
	в) Матожидание числа перезапусков.
	г) Общее время работы
5. Эксперименты
	а) общего времени алгоритма
	б) поэтапно
6. Выводы
	а) Пришлось предложить модифицированный EA-RL, чтобы алгоритм всегда заканчивал работу.
	б) EA+RL помог решить задачу, не решаемую RLS-ом, с помощью RLS. Это поможет в задачах с RLS выбираться из локальных оптимумов.
	в) На первом этапе заметно помогло наличие критерия, который помогал
	г) На втором этапе помогло обучение (не давало откатываться назад более двух раз)
	д) На третьем этапе обучение выродило задачу в RLS на OneMax
