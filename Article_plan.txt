1. Введение (аналогично OneMax+ZeroMax)
	а) Про многокритериалку
	б) EA+RL и про прошлые работы по нему
2. Постановка задачи
	а) Фитнес-функция и вспомогательные критерии
	б) описание алгоритма
3. Решение
	а) Разбиение хода алгоритма на 3 этапа
	б) Оценка времени работы первого этапа системой или дрифтом
	в) Рассуждение про окончание первого этапа: 2 варианта выбора оптимизируемого критерия (jump или right_bridge)
	г) Рассмотрение 2ого этапа при выборе right_bridge. Аналог Learning lemma.
	д) Рассмотрение 3его этапа при выборе right_bridge (должно занять пару строчек)
	е) Обоснование того, что при выборе jump алгоритм зациклится. 
	ж) Матожидание времени до перезапуска: второй этап проходится так же быстро, но надо показать, сколько итераций займет двойное падение и подъем назад
	з) Вероятность того, что при выборе right_bridge произойдет перезапуск.
	и) Матожидание числа перезапусков.
4. Эксперименты
	а) общего времени алгоритма
	б) поэтапно
5. Выводы
	а) EA+RL помог решить задачу, не решаемую RLS-ом, с помощью RLS. Это поможет в задачах с RLS выбираться из локальных оптимумов.
	б) На первом этапе заметно помогло наличие критерия, который 